{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T00:28:20.409119Z",
     "start_time": "2020-09-09T00:28:20.290391Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\";\n",
       "                var nbb_formatted_code = \"%load_ext nb_black\\n%load_ext autoreload\\n%autoreload 2\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext nb_black\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T00:28:20.672813Z",
     "start_time": "2020-09-09T00:28:20.411031Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"import matplotlib.pyplot as plt\\nimport numpy as np\\n\\nimport util\\n\\nfrom poisson import PoissonRegression\";\n",
       "                var nbb_formatted_code = \"import matplotlib.pyplot as plt\\nimport numpy as np\\n\\nimport util\\n\\nfrom poisson import PoissonRegression\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import util\n",
    "\n",
    "from poisson import PoissonRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T00:28:20.691527Z",
     "start_time": "2020-09-09T00:28:20.674897Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"lr = 1e-2\\ntrain_path = \\\"train.csv\\\"\\neval_path = \\\"valid.csv\\\"\\nsave_path = \\\"poisson_pred.txt\\\"\";\n",
       "                var nbb_formatted_code = \"lr = 1e-2\\ntrain_path = \\\"train.csv\\\"\\neval_path = \\\"valid.csv\\\"\\nsave_path = \\\"poisson_pred.txt\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-2\n",
    "train_path = \"train.csv\"\n",
    "eval_path = \"valid.csv\"\n",
    "save_path = \"poisson_pred.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-09T00:28:21.224766Z",
     "start_time": "2020-09-09T00:28:20.693196Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 0: 1.0\n",
      "Loss at step 1: -3.345432818856089\n",
      "Loss at step 2: -7.015867174289413\n",
      "Loss at step 3: -9.752027402597824\n",
      "Loss at step 4: -11.421307984331529\n",
      "Loss at step 5: -12.18722927998713\n",
      "Loss at step 6: -12.442682500254747\n",
      "Loss at step 7: -12.515887078431406\n",
      "Loss at step 8: -12.54583165885861\n",
      "Loss at step 9: -12.567362394573909\n",
      "Loss at step 10: -12.58670994334704\n",
      "Loss at step 11: -12.60485528228953\n",
      "Loss at step 12: -12.621988871034343\n",
      "Loss at step 13: -12.638188493190638\n",
      "Loss at step 14: -12.653513512484993\n",
      "Loss at step 15: -12.668017806505567\n",
      "Loss at step 16: -12.681751703013662\n",
      "Loss at step 17: -12.694762412038417\n",
      "Loss at step 18: -12.707094237246658\n",
      "Loss at step 19: -12.718788746236221\n",
      "Loss at step 20: -12.729884927202425\n",
      "Loss at step 21: -12.74041933699443\n",
      "Loss at step 22: -12.750426241805611\n",
      "Loss at step 23: -12.759937750971126\n",
      "Loss at step 24: -12.768983944155092\n",
      "Loss at step 25: -12.777592992157645\n",
      "Loss at step 26: -12.785791271558459\n",
      "Loss at step 27: -12.793603473411123\n",
      "Loss at step 28: -12.801052706203267\n",
      "Loss at step 29: -12.808160593298235\n",
      "Loss at step 30: -12.814947365074293\n",
      "Loss at step 31: -12.821431945976734\n",
      "Loss at step 32: -12.82763203669682\n",
      "Loss at step 33: -12.83356419168915\n",
      "Loss at step 34: -12.839243892236054\n",
      "Loss at step 35: -12.844685615264003\n",
      "Loss at step 36: -12.84990289811268\n",
      "Loss at step 37: -12.854908399452736\n",
      "Loss at step 38: -12.859713956543079\n",
      "Loss at step 39: -12.864330639012996\n",
      "Loss at step 40: -12.868768799348791\n",
      "Loss at step 41: -12.873038120258448\n",
      "Loss at step 42: -12.877147659081867\n",
      "Loss at step 43: -12.881105889407811\n",
      "Loss at step 44: -12.884920740052506\n",
      "Loss at step 45: -12.888599631548383\n",
      "Loss at step 46: -12.892149510285206\n",
      "Loss at step 47: -12.895576880439531\n",
      "Loss at step 48: -12.898887833822155\n",
      "Loss at step 49: -12.902088077767266\n",
      "Loss at step 50: -12.905182961180861\n",
      "Loss at step 51: -12.90817749886027\n",
      "Loss at step 52: -12.911076394190898\n",
      "Loss at step 53: -12.913884060320772\n",
      "Loss at step 54: -12.916604639908153\n",
      "Loss at step 55: -12.919242023532323\n",
      "Loss at step 56: -12.921799866852727\n",
      "Loss at step 57: -12.924281606596711\n",
      "Loss at step 58: -12.926690475451856\n",
      "Loss at step 59: -12.92902951593411\n",
      "Loss at step 60: -12.931301593299093\n",
      "Loss at step 61: -12.93350940755977\n",
      "Loss at step 62: -12.935655504669894\n",
      "Loss at step 63: -12.93774228692911\n",
      "Loss at step 64: -12.939772022662032\n",
      "Loss at step 65: -12.9417468552205\n",
      "Loss at step 66: -12.943668811354987\n",
      "Loss at step 67: -12.945539808998348\n",
      "Loss at step 68: -12.947361664502257\n",
      "Loss at step 69: -12.949136099364106\n",
      "Loss at step 70: -12.950864746479642\n",
      "Loss at step 71: -12.95254915595443\n",
      "Loss at step 72: -12.954190800504856\n",
      "Loss at step 73: -12.955791080477532\n",
      "Loss at step 74: -12.957351328513905\n",
      "Loss at step 75: -12.95887281388513\n",
      "Loss at step 76: -12.960356746520546\n",
      "Loss at step 77: -12.961804280751547\n",
      "Loss at step 78: -12.96321651879105\n",
      "Loss at step 79: -12.964594513967494\n",
      "Loss at step 80: -12.965939273730928\n",
      "Loss at step 81: -12.967251762447507\n",
      "Loss at step 82: -12.968532903997632\n",
      "Loss at step 83: -12.969783584191886\n",
      "Loss at step 84: -12.97100465301784\n",
      "Loss at step 85: -12.972196926730065\n",
      "Loss at step 86: -12.973361189794595\n",
      "Loss at step 87: -12.97449819669844\n",
      "Loss at step 88: -12.975608673633914\n",
      "Loss at step 89: -12.9766933200669\n",
      "Loss at step 90: -12.977752810197462\n",
      "Loss at step 91: -12.9787877943206\n",
      "Loss at step 92: -12.979798900094526\n",
      "Loss at step 93: -12.98078673372305\n",
      "Loss at step 94: -12.981751881058427\n",
      "Loss at step 95: -12.98269490863042\n",
      "Loss at step 96: -12.983616364606961\n",
      "Loss at step 97: -12.98451677969136\n",
      "Loss at step 98: -12.98539666796072\n",
      "Loss at step 99: -12.986256527649799\n",
      "Loss at step 100: -12.987096841884266\n",
      "Loss at step 101: -12.987918079367079\n",
      "Loss at step 102: -12.98872069502131\n",
      "Loss at step 103: -12.989505130592626\n",
      "Loss at step 104: -12.990271815214278\n",
      "Loss at step 105: -12.991021165937369\n",
      "Loss at step 106: -12.991753588228823\n",
      "Loss at step 107: -12.992469476439428\n",
      "Loss at step 108: -12.993169214244071\n",
      "Loss at step 109: -12.99385317505613\n",
      "Loss at step 110: -12.99452172241788\n",
      "Loss at step 111: -12.995175210368622\n",
      "Loss at step 112: -12.99581398379205\n",
      "Loss at step 113: -12.99643837874441\n",
      "Loss at step 114: -12.997048722764667\n",
      "Loss at step 115: -12.997645335168043\n",
      "Loss at step 116: -12.998228527324024\n",
      "Loss at step 117: -12.998798602919898\n",
      "Loss at step 118: -12.99935585821083\n",
      "Loss at step 119: -12.999900582257393\n",
      "Loss at step 120: -13.000433057151383\n",
      "Loss at step 121: -13.000953558230695\n",
      "Loss at step 122: -13.001462354284037\n",
      "Loss at step 123: -13.001959707746085\n",
      "Loss at step 124: -13.002445874883744\n",
      "Loss at step 125: -13.002921105974075\n",
      "Loss at step 126: -13.003385645474445\n",
      "Loss at step 127: -13.003839732185341\n",
      "Loss at step 128: -13.004283599406369\n",
      "Loss at step 129: -13.00471747508582\n",
      "Loss at step 130: -13.005141581964219\n",
      "Loss at step 131: -13.005556137712192\n",
      "Loss at step 132: -13.005961355063038\n",
      "Loss at step 133: -13.00635744194025\n",
      "Loss at step 134: -13.006744601580374\n",
      "Loss at step 135: -13.007123032651355\n",
      "Loss at step 136: -13.007492929366736\n",
      "Loss at step 137: -13.007854481595844\n",
      "Loss at step 138: -13.00820787497027\n",
      "Loss at step 139: -13.008553290986777\n",
      "Loss at step 140: -13.008890907106833\n",
      "Loss at step 141: -13.009220896853\n",
      "Loss at step 142: -13.009543429902234\n",
      "Loss at step 143: -13.009858672176366\n",
      "Loss at step 144: -13.010166785929814\n",
      "Loss at step 145: -13.010467929834707\n",
      "Loss at step 146: -13.010762259063505\n",
      "Loss at step 147: -13.011049925369283\n",
      "Loss at step 148: -13.011331077163721\n",
      "Loss at step 149: -13.011605859592947\n",
      "Loss at step 150: -13.0118744146113\n",
      "Loss at step 151: -13.01213688105314\n",
      "Loss at step 152: -13.012393394702698\n",
      "Loss at step 153: -13.01264408836217\n",
      "Loss at step 154: -13.012889091917998\n",
      "Loss at step 155: -13.013128532405508\n",
      "Loss at step 156: -13.013362534071895\n",
      "Loss at step 157: -13.013591218437666\n",
      "Loss at step 158: -13.013814704356562\n",
      "Loss at step 159: -13.014033108074035\n",
      "Loss at step 160: -13.014246543284314\n",
      "Loss at step 161: -13.014455121186119\n",
      "Loss at step 162: -13.014658950537049\n",
      "Loss at step 163: -13.014858137706728\n",
      "Loss at step 164: -13.015052786728681\n",
      "Loss at step 165: -13.015242999351058\n",
      "Loss at step 166: -13.015428875086156\n",
      "Loss at step 167: -13.015610511258865\n",
      "Loss at step 168: -13.015788003053999\n",
      "Loss at step 169: -13.015961443562547\n",
      "Loss at step 170: -13.01613092382695\n",
      "Loss at step 171: -13.016296532885317\n",
      "Loss at step 172: -13.016458357814717\n",
      "Loss at step 173: -13.016616483773474\n",
      "Loss at step 174: -13.016770994042584\n",
      "Loss at step 175: -13.016921970066194\n",
      "Loss at step 176: -13.01706949149122\n",
      "Loss at step 177: -13.017213636206128\n",
      "Loss at step 178: -13.017354480378831\n",
      "Loss at step 179: -13.017492098493832\n",
      "Loss at step 180: -13.017626563388513\n",
      "Loss at step 181: -13.017757946288693\n",
      "Loss at step 182: -13.017886316843407\n",
      "Loss at step 183: -13.018011743158945\n",
      "Loss at step 184: -13.018134291832176\n",
      "Loss at step 185: -13.01825402798315\n",
      "Loss at step 186: -13.018371015287036\n",
      "Loss at step 187: -13.018485316005341\n",
      "Loss at step 188: -13.018596991016512\n",
      "Loss at step 189: -13.018706099845868\n",
      "Loss at step 190: -13.01881270069491\n",
      "Loss at step 191: -13.018916850469989\n",
      "Loss at step 192: -13.019018604810407\n",
      "Loss at step 193: -13.0191180181159\n",
      "Loss at step 194: -13.019215143573534\n",
      "Loss at step 195: -13.019310033184063\n",
      "Loss at step 196: -13.01940273778771\n",
      "Loss at step 197: -13.019493307089395\n",
      "Loss at step 198: -13.019581789683471\n",
      "Loss at step 199: -13.019668233077889\n",
      "Loss at step 200: -13.019752683717888\n",
      "Loss at step 201: -13.019835187009171\n",
      "Loss at step 202: -13.019915787340588\n",
      "Loss at step 203: -13.019994528106361\n",
      "Loss at step 204: -13.020071451727803\n",
      "Loss at step 205: -13.020146599674609\n",
      "Loss at step 206: -13.020220012485698\n",
      "Loss at step 207: -13.020291729789575\n",
      "Loss at step 208: -13.020361790324314\n",
      "Loss at step 209: -13.020430231957073\n",
      "Loss at step 210: -13.02049709170323\n",
      "Loss at step 211: -13.02056240574508\n",
      "Loss at step 212: -13.020626209450151\n",
      "Loss at step 213: -13.020688537389159\n",
      "Loss at step 214: -13.020749423353514\n",
      "Loss at step 215: -13.020808900372511\n",
      "Loss at step 216: -13.02086700073013\n",
      "Loss at step 217: -13.020923755981489\n",
      "Loss at step 218: -13.020979196968925\n",
      "Loss at step 219: -13.021033353837762\n",
      "Loss at step 220: -13.021086256051708\n",
      "Loss at step 221: -13.021137932407951\n",
      "Loss at step 222: -13.021188411051915\n",
      "Loss at step 223: -13.021237719491694\n",
      "Loss at step 224: -13.021285884612205\n",
      "Loss at step 225: -13.021332932689006\n",
      "Loss at step 226: -13.021378889401817\n",
      "Loss at step 227: -13.021423779847785\n",
      "Loss at step 228: -13.02146762855442\n",
      "Loss at step 229: -13.021510459492282\n",
      "Loss at step 230: -13.021552296087368\n",
      "Loss at step 231: -13.02159316123326\n",
      "Loss at step 232: -13.021633077302994\n",
      "Loss at step 233: -13.02167206616067\n",
      "Loss at step 234: -13.021710149172808\n",
      "Loss at step 235: -13.021747347219481\n",
      "Loss at step 236: -13.021783680705171\n",
      "Loss at step 237: -13.021819169569422\n",
      "Loss at step 238: -13.021853833297234\n",
      "Loss at step 239: -13.021887690929251\n",
      "Loss at step 240: -13.021920761071712\n",
      "Loss at step 241: -13.021953061906196\n",
      "Loss at step 242: -13.021984611199155\n",
      "Loss at step 243: -13.022015426311222\n",
      "Loss at step 244: -13.022045524206339\n",
      "Loss at step 245: -13.022074921460655\n",
      "Loss at step 246: -13.02210363427127\n",
      "Loss at step 247: -13.022131678464747\n",
      "Loss at step 248: -13.022159069505465\n",
      "Loss at step 249: -13.022185822503767\n",
      "Loss at step 250: -13.02221195222395\n",
      "Loss at step 251: -13.022237473092066\n",
      "Loss at step 252: -13.022262399203548\n",
      "Loss at step 253: -13.022286744330687\n",
      "Loss at step 254: -13.022310521929917\n",
      "Loss at step 255: -13.02233374514896\n",
      "Loss at step 256: -13.0223564268338\n",
      "Loss at step 257: -13.022378579535534\n",
      "Loss at step 258: -13.022400215517004\n",
      "Loss at step 259: -13.022421346759366\n",
      "Loss at step 260: -13.022441984968438\n",
      "Loss at step 261: -13.022462141580975\n",
      "Loss at step 262: -13.02248182777074\n",
      "Loss at step 263: -13.022501054454487\n",
      "Loss at step 264: -13.0225198322978\n",
      "Loss at step 265: -13.02253817172078\n",
      "Loss at step 266: -13.022556082903641\n",
      "Loss at step 267: -13.02257357579215\n",
      "Loss at step 268: -13.022590660102965\n",
      "Loss at step 269: -13.022607345328847\n",
      "Loss at step 270: -13.022623640743753\n",
      "Loss at step 271: -13.022639555407821\n",
      "Loss at step 272: -13.022655098172248\n",
      "Loss at step 273: -13.022670277684034\n",
      "Loss at step 274: -13.022685102390659\n",
      "Loss at step 275: -13.022699580544622\n",
      "Loss at step 276: -13.022713720207891\n",
      "Loss at step 277: -13.022727529256253\n",
      "Loss at step 278: -13.022741015383572\n",
      "Loss at step 279: -13.022754186105935\n",
      "Loss at step 280: -13.022767048765726\n",
      "Loss at step 281: -13.022779610535595\n",
      "Loss at step 282: -13.02279187842233\n",
      "Loss at step 283: -13.02280385927067\n",
      "Loss at step 284: -13.022815559766997\n",
      "Loss at step 285: -13.022826986442977\n",
      "Loss at step 286: -13.022838145679096\n",
      "Loss at step 287: -13.02284904370812\n",
      "Loss at step 288: -13.022859686618494\n",
      "Loss at step 289: -13.022870080357645\n",
      "Loss at step 290: -13.022880230735213\n",
      "Loss at step 291: -13.022890143426219\n",
      "Loss at step 292: -13.022899823974157\n",
      "Loss at step 293: -13.022909277794001\n",
      "Loss at step 294: -13.02291851017518\n",
      "Loss at step 295: -13.02292752628443\n",
      "Loss at step 296: -13.022936331168658\n",
      "Loss at step 297: -13.022944929757653\n",
      "Loss at step 298: -13.02295332686681\n",
      "Loss at step 299: -13.022961527199753\n",
      "Loss at step 300: -13.022969535350896\n",
      "Loss at step 301: -13.022977355807976\n",
      "Loss at step 302: -13.022984992954505\n",
      "Loss at step 303: -13.022992451072149\n",
      "Loss at step 304: -13.022999734343117\n",
      "Loss at step 305: -13.02300684685241\n",
      "Loss at step 306: -13.02301379259009\n",
      "Loss at step 307: -13.023020575453458\n",
      "Loss at step 308: -13.023027199249196\n",
      "Loss at step 309: -13.023033667695465\n",
      "Loss at step 310: -13.023039984423935\n",
      "Loss at step 311: -13.023046152981788\n",
      "Loss at step 312: -13.023052176833678\n",
      "Loss at step 313: -13.02305805936362\n",
      "Loss at step 314: -13.023063803876868\n",
      "Loss at step 315: -13.023069413601728\n",
      "Loss at step 316: -13.023074891691339\n",
      "Loss at step 317: -13.023080241225413\n",
      "Loss at step 318: -13.023085465211924\n",
      "Loss at step 319: -13.023090566588788\n",
      "Loss at step 320: -13.023095548225454\n",
      "Loss at step 321: -13.023100412924517\n",
      "Loss at step 322: -13.023105163423256\n",
      "Loss at step 323: -13.023109802395135\n",
      "Loss at step 324: -13.0231143324513\n",
      "Loss at step 325: -13.023118756142008\n",
      "Loss at step 326: -13.023123075958049\n",
      "Loss at step 327: -13.023127294332111\n",
      "Loss at step 328: -13.023131413640145\n",
      "Loss at step 329: -13.023135436202665\n",
      "Loss at step 330: -13.023139364286038\n",
      "Loss at step 331: -13.023143200103744\n",
      "Loss at step 332: -13.023146945817603\n",
      "Loss at step 333: -13.023150603538964\n",
      "Loss at step 334: -13.023154175329893\n",
      "Loss at step 335: -13.0231576632043\n",
      "Loss at step 336: -13.02316106912907\n",
      "Loss at step 337: -13.023164395025153\n",
      "Loss at step 338: -13.023167642768623\n",
      "Loss at step 339: -13.023170814191728\n",
      "Loss at step 340: -13.023173911083914\n",
      "Loss at step 341: -13.023176935192797\n",
      "Loss at step 342: -13.023179888225172\n",
      "Loss at step 343: -13.023182771847924\n",
      "Loss at step 344: -13.023185587688983\n",
      "Loss at step 345: -13.023188337338217\n",
      "Loss at step 346: -13.023191022348325\n",
      "Loss at step 347: -13.023193644235693\n",
      "Loss at step 348: -13.023196204481248\n",
      "Loss at step 349: -13.023198704531278\n",
      "Loss at step 350: -13.023201145798247\n",
      "Loss at step 351: -13.02320352966156\n",
      "Loss at step 352: -13.023205857468364\n",
      "Loss at step 353: -13.023208130534279\n",
      "Loss at step 354: -13.023210350144133\n",
      "Loss at step 355: -13.02321251755269\n",
      "Loss at step 356: -13.023214633985342\n",
      "Loss at step 357: -13.023216700638793\n",
      "Loss at step 358: -13.023218718681738\n",
      "Loss at step 359: -13.023220689255503\n",
      "Loss at step 360: -13.023222613474688\n",
      "Loss at step 361: -13.023224492427799\n",
      "Loss at step 362: -13.023226327177838\n",
      "Loss at step 363: -13.023228118762919\n",
      "Loss at step 364: -13.02322986819683\n",
      "Loss at step 365: -13.023231576469616\n",
      "Loss at step 366: -13.023233244548123\n",
      "Loss at step 367: -13.023234873376543\n",
      "Loss at step 368: -13.023236463876941\n",
      "Loss at step 369: -13.023238016949776\n",
      "Loss at step 370: -13.023239533474399\n",
      "Loss at step 371: -13.023241014309557\n",
      "Loss at step 372: -13.023242460293858\n",
      "Loss at step 373: -13.023243872246255\n",
      "Loss at step 374: -13.023245250966498\n",
      "Loss at step 375: -13.02324659723559\n",
      "Loss at step 376: -13.023247911816213\n",
      "Loss at step 377: -13.02324919545317\n",
      "Loss at step 378: -13.023250448873792\n",
      "Loss at step 379: -13.023251672788353\n",
      "Loss at step 380: -13.023252867890466\n",
      "Loss at step 381: -13.02325403485747\n",
      "Loss at step 382: -13.02325517435082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 383: -13.02325628701645\n",
      "Loss at step 384: -13.023257373485132\n",
      "Loss at step 385: -13.023258434372844\n",
      "Loss at step 386: -13.023259470281104\n",
      "Loss at step 387: -13.023260481797319\n",
      "Loss at step 388: -13.023261469495102\n",
      "Loss at step 389: -13.023262433934612\n",
      "Loss at step 390: -13.023263375662848\n",
      "Loss at step 391: -13.023264295213979\n",
      "Loss at step 392: -13.023265193109623\n",
      "Loss at step 393: -13.023266069859162\n",
      "Loss at step 394: -13.023266925960007\n",
      "Loss at step 395: -13.023267761897898\n",
      "Loss at step 396: -13.023268578147164\n",
      "Loss at step 397: -13.023269375170997\n",
      "Loss at step 398: -13.023270153421707\n",
      "Loss at step 399: -13.023270913340989\n",
      "Loss at step 400: -13.023271655360151\n",
      "Loss at step 401: -13.023272379900378\n",
      "Loss at step 402: -13.023273087372962\n",
      "Loss at step 403: -13.02327377817953\n",
      "Loss at step 404: -13.023274452712265\n",
      "Loss at step 405: -13.023275111354153\n",
      "Loss at step 406: -13.023275754479158\n",
      "Loss at step 407: -13.023276382452485\n",
      "Loss at step 408: -13.023276995630734\n",
      "Loss at step 409: -13.023277594362135\n",
      "Loss at step 410: -13.023278178986727\n",
      "Loss at step 411: -13.023278749836567\n",
      "Loss at step 412: -13.023279307235896\n",
      "Loss at step 413: -13.023279851501345\n",
      "Loss at step 414: -13.02328038294209\n",
      "Loss at step 415: -13.023280901860051\n",
      "Loss at step 416: -13.023281408550043\n",
      "Loss at step 417: -13.023281903299948\n",
      "Loss at step 418: -13.02328238639088\n",
      "Loss at step 419: -13.023282858097355\n",
      "Loss at step 420: -13.023283318687419\n",
      "Loss at step 421: -13.02328376842282\n",
      "Loss at step 422: -13.023284207559152\n",
      "Loss at step 423: -13.023284636345998\n",
      "Loss at step 424: -13.02328505502707\n",
      "Loss at step 425: -13.023285463840354\n",
      "Loss at step 426: -13.023285863018227\n",
      "Loss at step 427: -13.02328625278761\n",
      "Loss at step 428: -13.023286633370084\n",
      "Loss at step 429: -13.023287004982013\n",
      "Loss at step 430: -13.023287367834678\n",
      "Loss at step 431: -13.023287722134386\n",
      "Loss at step 432: -13.02328806808259\n",
      "Loss at step 433: -13.023288405876004\n",
      "Loss at step 434: -13.023288735706714\n",
      "Loss at step 435: -13.023289057762286\n",
      "Loss at step 436: -13.023289372225873\n",
      "Loss at step 437: -13.02328967927632\n",
      "Loss at step 438: -13.023289979088258\n",
      "Loss at step 439: -13.023290271832215\n",
      "Loss at step 440: -13.0232905576747\n",
      "Loss at step 441: -13.023290836778308\n",
      "Loss at step 442: -13.023291109301805\n",
      "Loss at step 443: -13.02329137540022\n",
      "Loss at step 444: -13.023291635224933\n",
      "Loss at step 445: -13.023291888923763\n",
      "Loss at step 446: -13.023292136641047\n",
      "Loss at step 447: -13.023292378517725\n",
      "Loss at step 448: -13.023292614691423\n",
      "Loss at step 449: -13.023292845296517\n",
      "Loss at step 450: -13.02329307046423\n",
      "Loss at step 451: -13.023293290322691\n",
      "Loss at step 452: -13.023293504997017\n",
      "Loss at step 453: -13.02329371460937\n",
      "Loss at step 454: -13.023293919279045\n",
      "Loss at step 455: -13.02329411912252\n",
      "Loss at step 456: -13.02329431425354\n",
      "Loss at step 457: -13.023294504783161\n",
      "Loss at step 458: -13.023294690819833\n",
      "Loss at step 459: -13.023294872469446\n",
      "Loss at step 460: -13.023295049835397\n",
      "Loss at step 461: -13.023295223018655\n",
      "Loss at step 462: -13.023295392117802\n",
      "Loss at step 463: -13.023295557229103\n",
      "Loss at step 464: -13.02329571844655\n",
      "Loss at step 465: -13.023295875861931\n",
      "Loss at step 466: -13.023296029564868\n",
      "Loss at step 467: -13.023296179642868\n",
      "Loss at step 468: -13.023296326181379\n",
      "Loss at step 469: -13.023296469263842\n",
      "Loss at step 470: -13.02329660897172\n",
      "Loss at step 471: -13.023296745384572\n",
      "Loss at step 472: -13.023296878580071\n",
      "Loss at step 473: -13.023297008634067\n",
      "Loss at step 474: -13.023297135620616\n",
      "Loss at step 475: -13.023297259612042\n",
      "Loss at step 476: -13.02329738067895\n",
      "Loss at step 477: -13.02329749889029\n",
      "Loss at step 478: -13.02329761431339\n",
      "Loss at step 479: -13.023297727013983\n",
      "Loss at step 480: -13.023297837056264\n",
      "Loss at step 481: -13.0232979445029\n",
      "Loss at step 482: -13.0232980494151\n",
      "Loss at step 483: -13.023298151852616\n",
      "Loss at step 484: -13.023298251873802\n",
      "Loss at step 485: -13.02329834953563\n",
      "Loss at step 486: -13.023298444893731\n",
      "Loss at step 487: -13.023298538002425\n",
      "Loss at step 488: -13.023298628914757\n",
      "Loss at step 489: -13.023298717682518\n",
      "Loss at step 490: -13.023298804356271\n",
      "Loss at step 491: -13.0232988889854\n",
      "Loss at step 492: -13.023298971618123\n",
      "Loss at step 493: -13.02329905230151\n",
      "Loss at step 494: -13.023299131081538\n",
      "Loss at step 495: -13.02329920800309\n",
      "Loss at step 496: -13.02329928310999\n",
      "Loss at step 497: -13.023299356445035\n",
      "Loss at step 498: -13.02329942805001\n",
      "Loss at step 499: -13.023299497965715\n",
      "Loss at step 500: -13.023299566231994\n",
      "Loss at step 501: -13.02329963288774\n",
      "Loss at step 502: -13.02329969797094\n",
      "Loss at step 503: -13.023299761518683\n",
      "Loss at step 504: -13.023299823567179\n",
      "Loss at step 505: -13.023299884151788\n",
      "Loss at step 506: -13.02329994330704\n",
      "Loss at step 507: -13.023300001066644\n",
      "Loss at step 508: -13.023300057463517\n",
      "Loss at step 509: -13.023300112529801\n",
      "Loss at step 510: -13.023300166296881\n",
      "Loss at step 511: -13.0233002187954\n",
      "Loss at step 512: -13.023300270055278\n",
      "Loss at step 513: -13.023300320105735\n",
      "Loss at step 514: -13.023300368975292\n",
      "Loss at step 515: -13.023300416691807\n",
      "Loss at step 516: -13.023300463282478\n",
      "Loss at step 517: -13.023300508773863\n",
      "Loss at step 518: -13.023300553191888\n",
      "Loss at step 519: -13.023300596561874\n",
      "Loss at step 520: -13.023300638908543\n",
      "Loss at step 521: -13.023300680256032\n",
      "Loss at step 522: -13.023300720627914\n",
      "Loss at step 523: -13.023300760047201\n",
      "Loss at step 524: -13.023300798536365\n",
      "Loss at step 525: -13.023300836117347\n",
      "Loss at step 526: -13.023300872811571\n",
      "Loss at step 527: -13.023300908639959\n",
      "Loss at step 528: -13.02330094362293\n",
      "Loss at step 529: -13.023300977780433\n",
      "Loss at step 530: -13.023301011131938\n",
      "Loss at step 531: -13.023301043696463\n",
      "Loss at step 532: -13.023301075492572\n",
      "Loss at step 533: -13.023301106538394\n",
      "Loss at step 534: -13.023301136851627\n",
      "Loss at step 535: -13.023301166449555\n",
      "Loss at step 536: -13.023301195349053\n",
      "Loss at step 537: -13.023301223566602\n",
      "Loss at step 538: -13.023301251118289\n",
      "Loss at step 539: -13.023301278019819\n",
      "Loss at step 540: -13.023301304286537\n",
      "Loss at step 541: -13.023301329933416\n",
      "Loss at step 542: -13.023301354975084\n",
      "Loss at step 543: -13.023301379425817\n",
      "Loss at step 544: -13.023301403299557\n",
      "Loss at step 545: -13.023301426609917\n",
      "Loss at step 546: -13.023301449370189\n",
      "Loss at step 547: -13.023301471593356\n",
      "Loss at step 548: -13.023301493292086\n",
      "Loss at step 549: -13.023301514478753\n",
      "Loss at step 550: -13.023301535165443\n",
      "Loss at step 551: -13.023301555363947\n",
      "Loss at step 552: -13.023301575085785\n",
      "Loss at step 553: -13.02330159434221\n",
      "Loss at step 554: -13.023301613144191\n",
      "Loss at step 555: -13.023301631502465\n",
      "Loss at step 556: -13.02330164942749\n",
      "Loss at step 557: -13.023301666929493\n",
      "Loss at step 558: -13.023301684018458\n",
      "Loss at step 559: -13.023301700704128\n",
      "Loss at step 560: -13.023301716996018\n",
      "Loss at step 561: -13.023301732903423\n",
      "Loss at step 562: -13.023301748435411\n",
      "Loss at step 563: -13.023301763600847\n",
      "Loss at step 564: -13.023301778408376\n",
      "Loss at step 565: -13.02330179286644\n",
      "Loss at step 566: -13.02330180698329\n",
      "Loss at step 567: -13.02330182076698\n",
      "Loss at step 568: -13.023301834225366\n",
      "Loss at step 569: -13.023301847366126\n",
      "Loss at step 570: -13.023301860196753\n",
      "Loss at step 571: -13.023301872724572\n",
      "Loss at step 572: -13.023301884956721\n",
      "Loss at step 573: -13.02330189690018\n",
      "Loss at step 574: -13.023301908561763\n",
      "Loss at step 575: -13.023301919948121\n",
      "Loss at step 576: -13.023301931065745\n",
      "Loss at step 577: -13.023301941920982\n",
      "Loss at step 578: -13.02330195252002\n",
      "Loss at step 579: -13.023301962868906\n",
      "Loss at step 580: -13.023301972973547\n",
      "Loss at step 581: -13.023301982839703\n",
      "Loss at step 582: -13.023301992472998\n",
      "Loss at step 583: -13.023302001878934\n",
      "Loss at step 584: -13.02330201106287\n",
      "Loss at step 585: -13.023302020030053\n",
      "Loss at step 586: -13.02330202878559\n",
      "Loss at step 587: -13.023302037334483\n",
      "Loss at step 588: -13.023302045681605\n",
      "Loss at step 589: -13.023302053831715\n",
      "Loss at step 590: -13.023302061789467\n",
      "Loss at step 591: -13.023302069559396\n",
      "Loss at step 592: -13.023302077145942\n",
      "Loss at step 593: -13.023302084553425\n",
      "Loss at step 594: -13.023302091786073\n",
      "Loss at step 595: -13.023302098848015\n",
      "Loss at step 596: -13.023302105743275\n",
      "Loss at step 597: -13.023302112475793\n",
      "Loss at step 598: -13.023302119049402\n",
      "Loss at step 599: -13.023302125467858\n",
      "Loss at step 600: -13.02330213173482\n",
      "Loss at step 601: -13.023302137853866\n",
      "Loss at step 602: -13.023302143828484\n",
      "Loss at step 603: -13.023302149662081\n",
      "Loss at step 604: -13.023302155357994\n",
      "Loss at step 605: -13.023302160919462\n",
      "Loss at step 606: -13.023302166349664\n",
      "Loss at step 607: -13.023302171651695\n",
      "Loss at step 608: -13.023302176828581\n",
      "Loss at step 609: -13.023302181883277\n",
      "Loss at step 610: -13.023302186818665\n",
      "Loss at step 611: -13.02330219163756\n",
      "Loss at step 612: -13.023302196342716\n",
      "Loss at step 613: -13.023302200936811\n",
      "Loss at step 614: -13.023302205422471\n",
      "Loss at step 615: -13.023302209802255\n",
      "Loss at step 616: -13.023302214078658\n",
      "Loss at step 617: -13.023302218254123\n",
      "Loss at step 618: -13.023302222331033\n",
      "Loss at step 619: -13.023302226311712\n",
      "Loss at step 620: -13.023302230198434\n",
      "Loss at step 621: -13.023302233993412\n",
      "Loss at step 622: -13.023302237698813\n",
      "Loss at step 623: -13.023302241316754\n",
      "Loss at step 624: -13.023302244849297\n",
      "Loss at step 625: -13.023302248298457\n",
      "Loss at step 626: -13.023302251666205\n",
      "Loss at step 627: -13.023302254954459\n",
      "Loss at step 628: -13.023302258165096\n",
      "Loss at step 629: -13.023302261299952\n",
      "Loss at step 630: -13.02330226436081\n",
      "Loss at step 631: -13.023302267349418\n",
      "Loss at step 632: -13.023302270267484\n",
      "Loss at step 633: -13.02330227311667\n",
      "Loss at step 634: -13.023302275898603\n",
      "Loss at step 635: -13.02330227861487\n",
      "Loss at step 636: -13.023302281267021\n",
      "Loss at step 637: -13.02330228385657\n",
      "Loss at step 638: -13.023302286384993\n",
      "Loss at step 639: -13.023302288853735\n",
      "Loss at step 640: -13.023302291264205\n",
      "Loss at step 641: -13.023302293617773\n",
      "Loss at step 642: -13.023302295915789\n",
      "Loss at step 643: -13.02330229815956\n",
      "stopping early bcs weights diff: 9.94905441897842e-06\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"# Load training set\\nx_train, y_train = util.load_dataset(train_path, add_intercept=True)\\n\\n# *** START CODE HERE ***\\n# Fit a Poisson Regression model\\n# Run on the validation set, and use np.savetxt to save outputs to save_path\\n\\nmodel = PoissonRegression(lr)\\nmodel.fit(x_train, y_train)\\n\\nx_val, y_val = util.load_dataset(eval_path, add_intercept=True)\\nutil.plot(\\n    x_val, y_val, model.predict(x_val), save_path=save_path.replace(\\\".txt\\\", \\\".png\\\")\\n)\\n\\nnp.savetxt(save_path, model.predict(x_val))\\n# *** END CODE HERE ***\";\n",
       "                var nbb_formatted_code = \"# Load training set\\nx_train, y_train = util.load_dataset(train_path, add_intercept=True)\\n\\n# *** START CODE HERE ***\\n# Fit a Poisson Regression model\\n# Run on the validation set, and use np.savetxt to save outputs to save_path\\n\\nmodel = PoissonRegression(lr)\\nmodel.fit(x_train, y_train)\\n\\nx_val, y_val = util.load_dataset(eval_path, add_intercept=True)\\nutil.plot(\\n    x_val, y_val, model.predict(x_val), save_path=save_path.replace(\\\".txt\\\", \\\".png\\\")\\n)\\n\\nnp.savetxt(save_path, model.predict(x_val))\\n# *** END CODE HERE ***\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load training set\n",
    "x_train, y_train = util.load_dataset(train_path, add_intercept=True)\n",
    "\n",
    "# *** START CODE HERE ***\n",
    "# Fit a Poisson Regression model\n",
    "# Run on the validation set, and use np.savetxt to save outputs to save_path\n",
    "\n",
    "model = PoissonRegression(lr)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "x_val, y_val = util.load_dataset(eval_path, add_intercept=True)\n",
    "util.plot(\n",
    "    x_val, y_val, model.predict(x_val), save_path=save_path.replace(\".txt\", \".png\")\n",
    ")\n",
    "\n",
    "np.savetxt(save_path, model.predict(x_val))\n",
    "# *** END CODE HERE ***"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229-ps1",
   "language": "python",
   "name": "cs229-ps1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
